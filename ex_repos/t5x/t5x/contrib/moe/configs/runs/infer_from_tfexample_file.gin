# Mixture of Experts defaults for infer.py if using a TFExample file as input.
#
#
# The features from each TFExample are tokenized using the model's vocabulary.
# By default, the inputs feature is assumed to be keyed as 'inputs', but this
# can be overridden with `create_task_from_tfexample_file.inputs_key`.
#
# You must include a binding for MODEL.
#
# Required to be set:
#
# - TF_EXAMPLE_FILE_PATHS: The path to read TF Examples from.
# - TF_EXAMPLE_FILE_TYPE: The type of file to read TF Examples from. Currently
#                         supported: 'tfrecord', 'recordio', 'sstable'.
# - NUM_MODEL_PARTITIONS or MODEL_PARALLEL_SUBMESH (only specify one)
# - FEATURE_LENGTHS: The maximum length per feature in the TF Examples.
# - CHECKPOINT_PATH: The model checkpoint to use for inference
# - INFER_OUTPUT_DIR: The dir to write results to.
#
# See also t5x/configs/runs/infer_from_tfexample_file.gin for commonly
# overridden options.

from __gin__ import dynamic_registration

import __main__ as infer_script
from t5x.contrib.moe import partitioning as moe_partitioning

include 't5x/configs/runs/infer_from_tfexample_file.gin'

infer_script.infer.partitioner = @moe_partitioning.MoePjitPartitioner()

# One, and only one, of these should be specified.
NUM_MODEL_PARTITIONS = 1
MODEL_PARALLEL_SUBMESH = None

# Override to decrease the number of expert partitions. This is only an upper
# bound. Must be <= NUM_EXPERTS. Fewer expert partitions places more experts on
# the same device, requiring more expert replicas and greater memory overhead,
# but will reduce all-to-all communication costs.
NUM_EXPERT_PARTITIONS = %NUM_EXPERTS

# We use the MoE partitioner.
train_script.precompile.partitioner = @moe_partitioning.MoePjitPartitioner()
moe_partitioning.MoePjitPartitioner:
  num_expert_partitions = %NUM_EXPERT_PARTITIONS
  num_partitions = %NUM_MODEL_PARTITIONS
  model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
